---
title: "Predicting interest rates at the Lending Club"
author: "Marie Cordes"
date: "5/10/2020"
image: pic10.jpg
output:
  html_document:
    theme: cerulean
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, load_libraries, include = FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for timeseries operations
library(car) #allow for vif and heteroskedasticity tests
library(lmtest)
library(vroom)
```


# Introduction

The goal of this analysis is to walk through the mechanics of designing a predictive model using linear regression. The example we will use is Lending Club, a peer-to-peer lender. The goal is to come up with an algorithm that recommends the interest rate to charge to new loans. The lending club has made historical data publicly available and we will use it for this exercise.


# Load and prepare the data

We start by loading the data to R in a dataframe.

```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv(here::here("data", "LendingClub Data.csv"), skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data. 

```{r}
glimpse(lc_raw) 

lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean) 
```

The data is now in a clean format stored in the dataframe "lc_clean." 

# Exploring the data with some visualizations. 

```{r, data_visualisation}
#  Histogram of interest rates. 
ggplot(lc_clean, aes(x=int_rate))+
  geom_histogram(binwidth = 0.01)+
  theme_bw()+
  scale_x_continuous(labels=scales::percent)+
  theme(legend.position = "none")+
  labs(
    title = "Interest Rate Distribution",
    x= "Interest Rate", 
    y="Count")+
  NULL
```

The histogram seems to be fairly normally distributed. There seem to be gaps in frequency of certain interest rates (so not the usual smooth,continuous looking function we would hope for) but this could be due to fixed ranges of interest rates that are charged. The mode is around 11%. 

```{r,data_visualisation_1}
# Histogram of interest rates, different colours per term (36 or 60)
ggplot(lc_clean, aes(x=int_rate, fill=term))+
  geom_histogram(binwidth = 0.01)+
  theme_bw()+
  scale_x_continuous(labels=scales::percent)+
  labs(
    title = "Loan for 36 or 60 months?",
    x= "Interest Rate", 
    y="Count",
    fill = "Loan term (months)"
  )+
  NULL
```

This graph reveals that there are generally more 36-months loans than 60-months loans. Higher and lower interest rates are spread relatively evenly between the two although only very few 60-months loans have an interest rate below 10%.


```{r,data_visualisation_2 }
# Histogram of interest rates but use different color for loans of different grades 
ggplot(lc_clean, aes(x=int_rate, fill=grade))+
  geom_histogram(binwidth = 0.01)+
 theme_bw()+
   scale_x_continuous(labels=scales::percent)+
  labs(
    title = "Higher grade, lower interest",
    x= "Interest Rate", 
    y="Count",
    fill = "Grade"
  )+
  NULL
```


```{r,data_visualisation_3 }
# Density plot with colour for different grades.
ggplot(lc_clean, aes(x=int_rate, fill=grade, alpha = 0.2))+  
  geom_density()+
  facet_grid(rows = vars(grade))+
  theme_bw()+
  theme(legend.position = "none")+
  scale_x_continuous(labels = scales::percent)+ 
  labs(title = "Higher grade, lower interest 2.0",
       x="Interest Rate",
       y = "Density")+
  NULL
```

The histogram and the density plot above reveal insights into the division of interest rates by grade rating. The lending club seems to give out mostly grade A, B and C loans. There seems to be a negative relationship between grade rating and interest rates where the higher the grade, the lower the interest charged.

```{r,data_visualisation_4 }
# Scatter plot of loan amount against interest rate and with  the line of best fit
ggplot(lc_clean, aes(x=loan_amnt, y=int_rate))+
  geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method="lm", se = 0)+
  theme_bw()+
  scale_y_continuous(labels=scales::percent)+
  scale_x_continuous(labels=scales::dollar_format())+
  labs(
    title = "More money, higher interest ",
    x= "Loan Amount", 
    y="Interest Rate"
  )+
  NULL
```

We can see a positive correlation between loan amount and interest rate, indicating, the higher the loan amount, the higher the interest rate.

```{r,data_visualisation_5 }
# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 
ggplot(lc_clean, aes(x=annual_inc, y=int_rate))+
  geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method="lm", se = 0)+ 
  theme_bw()+
  scale_y_continuous(labels=scales::percent)+
  #reduce scale to see stronger result (sacrificed 1 data point)
  scale_x_continuous(labels=scales::dollar_format(),limits=c(0,2100000))+ 
  labs(
    title = "Earn more, pay more",
    x= "Annual Income", y="Interest Rate")+
  NULL
```

The graph seems to reflect a positive correlation between income and interest rate, indicating, the higher the income, the higher the interest rate to pay for a loan.  


```{r,data_visualisation_6 }
# In the same axes, produce box plots of the interest rate for every value of delinquencies
ggplot(lc_clean, aes(x=delinq_2yrs, y=int_rate, color=delinq_2yrs))+  
  geom_boxplot()+
  theme_bw()+
  scale_y_continuous(labels=scales::percent)+
  theme(legend.position = "none")+
  labs(
    title = "Do delinquencies in the last two years have an effect \non interest rate charged?",
    x= "Number of delinquecies in last two years", y="Interest Rate"
  )+
  NULL

```

There seems to be a positive correlation between interest rates and loans with 0-4 delinquencies in 2 years. However, there seems to be an inverse correlation from 4-6. There are few loan observations with 7-8 delinquencies and even fewer observations of over 8 delinquencies which makes the interpretation more difficult. This analysis may benefit from re-categorizing.

``` {r,data_visualisation_7}
#boxplot with colour for different home_ownerhsip
lc_clean$home_ownership <- factor(lc_clean$home_ownership, levels = c("RENT", "OTHER", "MORTGAGE", "OWN", "NONE"))

ggplot(lc_clean, aes(x=home_ownership, y=int_rate, colour=home_ownership))+  
  geom_boxplot()+
  theme_bw()+
  theme(legend.position = "none")+
  scale_y_continuous(labels=scales::percent)+
  labs(title = "Does Home Ownership have an influence on interest rate charged?",
    y="Interest Rate", 
    x="Home Ownership")+
  NULL
```

We cannot find a clear relationship of the influence of the kind of home ownership the people taking on a loan have with the interest rates they will be charged.

## Scatterplot- Correlation Matrix

We build a correlation table for the numerical variables and investigate the impact of a number of variables on the interest rate charged graphically

```{r, correlation}
# correlation table using GGally::ggpairs()
# this takes a while to plot
lc_clean %>% 
  select(term, loan_amnt, dti,annual_inc, int_rate) %>% #keep Y variable last
  ggpairs(aes(alpha = 0.2, colour = term)) +
  theme_bw()
```

# Estimate simple linear regression models

We start with a simple but quite powerful model.

```{r, simple regression}
#Use the lm command to estimate a regression model with the following variables "loan_amnt",  "term", "dti", "annual_inc", and "grade"

model1<-lm( int_rate~loan_amnt+term+dti+annual_inc+grade, data=lc_clean)
  
summary(model1)
ncvTest(model1) # check inferential capabilities of this model/heteroskedasticity
vif(model1)  # check inferential capabilities of this model/multicolinearity
crPlots(model1)

```

## Q2. Questions on model 1 {-}

a. Are all variables statistically significant?
b. How do we interpret the coefficient of the Term60 dummy variable? Of the grade B dummy variable?
c. How much explanatory power does the model have?
d. Approximately, how wide would the 95% confidence interval of any prediction based on this model be?


>a. Yes, all explanatory variables, except annual_income, are statistically significant because the t-value >> |2| (Using 2 as a 5% benchmark).
b. The model assumes a baseline term of 36 months, The term60 coefficient is added to the interest rate estimate if the underlying loan has a term structure of 60 months. The coefficient of term60 is 3.608e-03, this implies that compared to a 36 month loan, the interest rate of a 60 month loan will be on average 0.36% higher. The model further assumes a baseline grade A quality. The coefficient of grade B loans is 3.554e-02, which implies that compared to a grade A loan, the interest rate of a grade B loan will be 3.55% higher.
c. The model captures/explains 91.97% of the variability of interest rates, as shown by the adjusted R-squared.
d. The 95% confidence interval for any prediction will be (+/- 2*RSE). With an RSE of 0.01056, the 95% confidence interval is (-2.112, 2.112), meaning a prediction width of 4.224%.


# Feature Engineering

Let's build progressively more complex models with more features.

```{r, Feature Engineering}
#Add to the previous model an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <-lm(int_rate ~ loan_amnt + term+dti + annual_inc + grade + loan_amnt*grade, data=lc_clean)
summary(model2)

#Add to the model the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <- lm(int_rate ~ loan_amnt + term + dti + grade + loan_amnt*grade + poly(annual_inc,3), data=lc_clean)
summary(model3)

#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables.
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <-lm(int_rate ~ loan_amnt + term + dti + grade + loan_amnt*grade + quartiles_annual_inc, data=lc_clean)
summary(model4)  

#Compare the performance of these four models using the anova command
anova(model1, model2, model3, model4)

#Presentation of all results
huxreg(model1, model2, model3, model4)

  
```

## Q3. Questions on models 1-4 {-}
a. Which of the four models has the most explanatory power in sample?
b. In model 2, how do we interpret the estimated coefficient of the interaction term between grade B and loan amount?
c. The problem of multicolinearity describes the situations where one feature is highly correlated with other features (or with a linear combination of other features). If our goal is to use the model to make predictions, should we be concerned by the problem of multicolinearity? Why, or why not?

>a. The adjusted R-Squareds of all 4 models are all approx. at 92% explained variance and very very similar. Model 2, 3 and 4 have the same, highest adjusted R-squared of 0.9204. Nevertheless, the lowest adjusted R-squared among the models is 0.9197 (model1). The difference is rather insignificant. Considering the RSS, we find that model 4 has the lowest RSS at 4.1850. Model 2 and 3's RSS are very similar while also model1's RSS of 4.2192 is not significantly larger. Strictly comparing numbres, model4 has the highest explanatory power. However, as there are no signficiant differences in explanatory power between the four models, for simiplicity of understanding (less complex) and because it is still a very powerful statistical model with 91.97% explanatory power, model1 is preferable. We also note that model1 has no multiocolinearity issues which would mean this model could be usued for causal inference. However, after running a Breusch-Pagan test, we reject the null and find evidence of heteroskedasticity which affects the causal inferential capabilities of the model but not its predictive power. 
b. The coefficient of the interaction term loan amount:gradeB is -6.617e-08. This implies that on average (1.528e-07 - 6.617e-08 = 8.663e-08) an additional $100K of loan will increase the interest rate of a grade B loan by 0.8663%.
c. For the purpose of prediction multicolinearity is not an issue, unlike in inference. This is because multicolinearity can cause issues in the Standard Error of the coeffiencients and coefficients estimates values, however removing multicolinearity does not alter the predictive power/capacity of the overall model, both models(with and without multicolinearity issues) will produce identical results for fitted values and prediction intervals. 


# Out of sample testing
Checking the predictive accuracy of model2 by holding out a subset of the data to use as testing. This method is sometimes referred to as the hold-out method for out-of-sample testing.

```{r, out of sample testing}
#split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.

set.seed(1456)
train_test_split <- initial_split(lc_clean, prop = 0.8)
training <- training(train_test_split)
testing <- testing(train_test_split)



#Fit model2 on the training set 
model2_training<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt, training)


#Calculate the RMSE of the model in the training set (in sample)
rmse_training<-sqrt(mean((residuals(model2_training))^2))
#USe the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)
#Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- RMSE(pred,testing$int_rate)


data.frame(RMSE_Training=rmse_training,RMSE_Testing=rmse_testing)

rmse_training # = 0.01053025  (in-sample)
rmse_testing # = 0.01045298   (out-of-sample)
(rmse_testing/rmse_training-1) # = -0.007338007

```

## Q4. Question on hold-out-method model {-}

**How much does the predictive accuracy of model 2 deteriorate when we move from in sample to out of sample testing? Is this sensitive to the random seed chosen? Is there any evidence of overfitting?**

>The predictive accuracy of the model does not deteriorate since the RMSE decreases by approx. 0.7% from 0.01053025 to 0.01045298 from the training to the testing set respectively. As the RMSE measures the error of our model in predicting data, we see that the smaller RMSE means that the predictive accuracy increases as we move from in sample to out of sample testing. The difference in RMSEs is, however very small, which implies that the model has a good fit in genral across both the training and testing data, again implying that it has good predictive capacity. Nevertheless, we cannot rule out that, with a different seed set, the RMSE may not be decresasing from in- to out-of-sample testing.

>This method/model is sensitive to the random seed chosen as setting a different seed, changes the resulting values slightly. Setting a seed helps with reproducing the model. However, fixing the training and testing data can create differences in the predictive capacity of the model as the random allocation of data points into a training and testing set can skew the model's predictive accuracy if the training and testing data are not well diversified enough. 

>If the RMSE of the testing set is very different from that of the training set, this would be an indication of overfitting the data. The RMSE of the testing set is only 0.7% smaller than that of the training set; assuming a threshold of 10% (minimial difference at which we would conclude overfitting), we find no signs of overfitting. 

# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation. Using the caret package this is easy.

```{r, k-fold cross validation, message=FALSE}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation
plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  
summary(plsFit)

```

## Q5. Question on k-fold method model {-}

**Compare the out-of-sample RMSE of 10-fold cross validation and the hold-out method. Are they different? Which do we think is more reliable? Are there any drawbacks to the k-fold cross validation method compared to the hold-out method? **

>The out-of sample (testing set) RMSE of the hold-out method is 0.01045298 while the 10-fold cross validation method yields an RMSE of 0.01052. These values do not differ very significantly as the k-fold RMSE is approx. 0.6% greater than the hold-out RMSE. 

>While the simplest way for cross validation is the hold-out method, the k-fold cross validation method is a way to improve it. In the k-fold method, each data point will be included in a testing set once and will be included k-1 times in the training set. This is beneficial as it is less relevant how the data is divided. Additionally, with increasing k, we reduce the estimated coefficient’s variance. A drawback of the k-fold cross validation compared to the hold out method is that the k-fold method takes k times, here 10 times (as we used 10 folds/k = 10), as much computation to reach a result as its training alogrithm has to be rerun from the very beginning k times (here, 10 times).


# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger datasets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves}
#select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  #traing the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```
```{r, learning_curve 2}
set.seed(124455454)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  #traing the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

## Q6. Question on learning curves {-}

Using the learning curves above, approximately how large of a sample size would we need to estimate model 3 reliably? Once we reach this sample size, if we want to reduce the prediction error further what options do we have?

>As n increases the model estimation will become more reliable and we expect the root mean square error (RMSE) of out of sample predictions to decrease and the out of sample R-squared to increase. For some n large enough, there will be little further gains from increasing n. In other words, to continue improving the performance of the model collecting more rows of the same features will not help. In the first example (set.seed(12)) we find this threshold to be approx. 1000, indicating a sample size of 1000 would be enough to estimate model 3 reliably. However, this is subject to the set seed as we see that setting the seed at 124455454 significantly changes the picture and the resulting threshold. Here, the learning curve indicates that we would need a sample size of around 3400 or perhaps >4000 to estimate model 3 reliably with smallest possible RMSE and highest possible R-squared. Taking these differences into account, we would recommend a sample size of at least 3000, potentially even around 4200 to estimate model 3 reliably.

>Once we would reach this sample size, instead of increasing the sample size and reducing prediction error we will need more features/more variables or would have to try transformations of the independent and/or dependent variables in order to further improve the model's accuracy. Another option to enhance the model's prediction accuracy is to use a regularization method such as LASSO.

# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

We try to estimate large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting}

#split the data in testing and training. The training test is really small.
set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient to a value that is different from zero. The penalty is proportional to a parameter $\lambda $ (pronounced lambda). This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter) and will be selected through k-fold cross validation in order to provide the best out-of-sample performance.  As result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficients and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization. 

```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
# set a seed to establish consistent results
set.seed(9999) 

#we will look for the optimal lambda in this sequence (we will try 1000 different lambdas)
lambda_seq <- seq(0, 0.01, length = 1000)
#lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
#Best lambda
lasso$bestTune$lambda
# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
#sum(coef(lasso$finalModel, lasso$bestTune$lambda)>0) =22 checks strictly greater than
#sum(coef(lasso$finalModel, lasso$bestTune$lambda)<0)= 5 checks strictly less than
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Q7. Questions on LASSO regression {-}
a. Which model performs best out of sample, OLS regression or LASSO?
b. What value of lambda offers best performance? Is this sensitive to the random seed?
c. How many coefficients are zero and how many are non-zero in the LASSO model of best fit?
d. Why is it important to standardize continuous variables before running LASSO?

>These results are subject to the seed that we set in the beginning of the LASSO regression.

>a. The OLS yields an out of sample RMSE of 0.0445857 and an R-squared of 0.3516455	while the LASSO method yields an RMSE of 0.01086846	and R-squared of 0.9176526. Comparing those, the OLS RMSE is much larger than that of the LASSO model, while, consequently, the R-squared of the LASSO model is significantly larger than the OLS R-squared. This implies that the LASSO regression explains significantly more variance and decreases the prediction error further, meaning that the LASSO method performs better than the OLS method due to a smaller RMSE and higher R-squared. 
b. The best lamda is 0.0005005005, re-running the chunk without a seed or with a different seed will yield a different result, so this value is sensitive to the random seed.
c. The number of coefficients that are not equal to zero is 27 (22 greater than zero and 5 less than zero), while the total number of coeffiecients equal to zero is 31.
d. Coefficients of different variables are expressed and measured in different units/at different scales and therefore, we need to standardize the continous, independent variables by subtracting the mean and dividing by the standard deviation. LASSO puts constraints on the coefficient's size so that the results depend on the magnitude of each variable. The goal is to rescale an original variable so that we can work with an equal variances/ranges across variables. Otherwise results could be misleading.


# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?
 
First, we investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can we use this information to further improve the forecasting accuracy of our model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time trends}

#linear time trend (add code below)
ggplot(lc_clean, aes(x=issue_d,y=int_rate))+
  geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method = "lm", se = 0)+
  theme_bw()+
  scale_y_continuous(labels=scales::percent)+
   labs(
    title = "Higher interest rates over time (?)",
    x= "Loan issuance month", 
    y="Interest rate"
  )+
  NULL

#linear time trend by grade (add code below)
ggplot(lc_clean, aes(x=issue_d,y=int_rate, color=grade))+
    geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method = "lm", se = 0)+
  theme_bw()+
  scale_y_continuous(labels=scales::percent)+
   labs(
    title = "Higher interest rates over time? - Not for grade A",
    x= "Loan issuance month", 
    y="Interest rate",
    color = "Grade"
  )+
  NULL

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ loan_amnt + term + dti + annual_inc + grade + issue_d,# using model 1 + issue_d
  lc_clean,
  method = "lm",
  trControl = control)

summary(time1)

#The second model has a different linear time trend for each grade class
time2<-train(
    int_rate ~ loan_amnt + term + dti + annual_inc + grade + issue_d + grade*issue_d, # adding interaction term between issue date and grade
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))



time3<-train(
    int_rate ~ loan_amnt + term + dti + annual_inc + grade + yq, #using model 1 + quarter dummy variable
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
  
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~ loan_amnt + term + dti + annual_inc + grade + yq + grade*yq, 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )

summary(time4)

data.frame(
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)

data.frame(
summary(time1)$adj.r.squared,
summary(time2)$adj.r.squared,
summary(time3)$adj.r.squared,
summary(time4)$adj.r.squared)

```

## Q8. Question on time series models {-}

**Based on our analysis above, is there any evidence to suggest that interest rates change over time? Does including time trends /quarter dummies improve predictions?**

>Graphically, the first graph shows a very slight increase of the interest rate over time. Differentiating interest rates by grade gives a clearer picture into the trends of interest rates over time. As we have seen before, the better the grade, the lower the interest rate overall. Additionally, we now see that the interest rates of grade A loans have actually decreased over time while the interest rates of all other loan grades have increased over time. Grade B only increases slightly, while the interest rates of lower grade loans increase more significantly, especially grades G, F and E.

>Using these insights for optimizing our models, we assessed the signifiance of time (issue_d) on our previously created and preferred model (model1). In time1 we find that issue_d is statistically significant (t-value 39.051 >>|2|) this slighly improved our model's adjusted R-squared and reduces the RMSE, improving our model overall. Including an interaction variable in time2 highlights the significance of time (issue_d) per grade as each variable is statistically significant, this again leads to an increase in explanatory power. The implementation of a dummy variable for quarters in time3 improved time1's predictive power by an almost 2% increase. However, using the quarter dummy variable and interaction terms in time4 yields the strongest result (amongst all our models) with an RMSE of 0.007614248	and adjusted R-squared of 0.958393. Technically speaking, the time trends and even more so the quarter dummies improve our model as the RSME decreases and the adjusted R-squared increases. Considering if adding time variables to our model is sensible makes us realize that time is not a very reliable factor for prediction as it often (but most importantly not always!) correlates with eonomic conditions which can drastically change - so that real-life predictions must take into account other factors from the economic environment rather than time in order to improve those predictions. Additionally, a specific time input will not be a yield a good prediction since attempting to predict the interest rate in the future will, most probably, not depend on whether someone takes on a loan on, e.g., March 5th or Arpil 5th. While our model has improved in explaining the variance of our data, including time trends and quarter dummies is not a good way to predict future interest rates.


# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it's better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month.


```{r, bond yields}
#load the data to memory as a dataframe
bond_prices<-vroom::vroom(here::here("data", "MonthBondYields.csv"))


#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+
  geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method = "lm", se = 0)+
  theme_bw()+
   labs(
    title = "Descreasing yields over time",
    x= "Year", 
    y="Price"
  )+
  NULL

#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds %>%
  ggplot(aes(x=int_rate, y=Price))+
  geom_point(size = 0.1, alpha = 0.5)+
  geom_smooth(method = "lm", se = 0)+
  theme_bw()+
  scale_x_continuous(labels=scales::percent)+
   labs(
    title = "Interest rates vs. bond prices",
    x= "Interest rate", 
    y="Price"
  )+
  NULL

lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm", se = 0) +
  theme_bw()+
  scale_x_continuous(labels=scales::percent)+
   labs(
    title = "Interest rates vs. bond prices by grade",
    x= "Interest rate", 
    y="Price",
    color = "Grade"
  )+
  NULL

#let's train a model using the bond information


plsFit<-train(
    int_rate ~ loan_amnt+term+dti+annual_inc+grade+Price , #model 1+ price 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
summary(plsFit)



```

## Q9. Question on bond yields {-}

**Do bond yields have any explanatory power?**

> Bond prices and yields are great indicators of the economy as a whole (in particular of inflation). Bond price and yield have an inverse relationship, meaning the higher a bond's price, the lower its yield. By plotting `Price` and incorporating the variable into our model, we can make assumptions about the explanatory power of both bond price and yield. 

>By plotting bond price against time we see there is a decreasing trend of bond price over time and, after plotting Price vs. interest rate, that, thus, the cost of borrowing is actually decreasing over time, i.e we would expect lower interest rates over time. Bond prices are also a good indicator of market conditions over time and thus would incorporate information about changes over time without having to use an explicit time vector that would be very difficult to make predictions on. Exploring the relationship of Prices vs Interest rates we notice bonds have a slight inverse relationship to interest rates i.e when the cost of borrowing money rises, bond prices usually fall, and vice-versa. The first graph only demonstates that this relationship is fairly weak and might not hold over all bond types. By filtering for grade we see a much clearer invese relationship between bond price and interest rates across bond grades B-G, however we note that there is actually a positive reltionship for grade A bonds. The rational for this invese relationship is quite logical as most bonds pay a fixed interest rate that becomes more attractive if interest rates fall, driving up demand and the price of the bond. Conversely, if interest rates rise, investors will no longer prefer the lower fixed interest rate paid by a bond, resulting in a decline in its price. It is interesting to note that grade A bonds do not obey this rule and actually become more favourable with a higher interest rate, this may be due to the surety of return that grade A bonds offer.

>As we can see a clear relationship exists, so we include bond prices as an explanatory variable in our model which proved to be significant (t-value = 60.526 >> |2|). This indicates that bond price and, thus, bond yields have high explanatory power for interest rates. The model also has a higher adjusted R-squared of 0.9268 and lower residual std error of 0.01008 than model1, so we can conclude that is is indeed a stronger model and reveals more meaningful results (that could be used for prediction) than the time series models (where we cannot extrapolated data into the future reliably) .

# Final model

## Q10. We choose a model and describe our methodology. {-}

```{r, final_model}
# final model: model1 + Price + interaction between Price and grade + eliminate annual income

finalmodel<-train(
    int_rate ~ loan_amnt+term+dti+grade+Price+ Price*grade, 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )

summary(finalmodel)
```

After investigating more models with different features using the methodologies covered so far, we present the model we believe predicts interest rates the best. We describe the chosen methodology and how good it is (including the approximate length of the 95% Confidence Interval of predictions that use this model) and what features it uses. (We do not use time trends or quarter dummies in our model as the first cannot be extrapolated into the future reliably and the second cannot be even estimated for future quarters.)

>Above is the model we believe, at this point, predicts interest rates the best and most reliably as all explanatroy variables are highly significant (t-value>>|2|) and no time trends or quarter dummy variables are added.

>The 95 % confidence interval for any prediction will be (+/- 2* RSE), (-0.019214, 0.019214), meaning an error width of 3.8% in interest rate estimate. This is smaller than  the predicition interval in model 1. We have an adjusted R-squared of 0.9335 and residual standard error of 0.009607 which suggests very high explanatory power. 

>We use a 10 fold cross validation method which is not subject to random seed errors. Our model uses an interaction term between grade and bond price as we saw different relationships (question 9) between interest rates and bond price amongst different grades. We eliminated annual income as an explanatory variable as it was not statistically significant in model 1 or our final model, and we are left with a model with highly significant explanatory variables.



## Q11. Exploring the effects of CPI and Inflation on the model {-}

We use other publicly available datasets to further improve performance (e.g., quarterly data on US inflation or [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N)) and explain why and check if the additional data will make a difference.

Additionaly, more data - especially more data on the economic and business environment may significantly improve our model as it provides more information that can explain more of the variance of our model.


```{r, final_model_plus_cpi}
#load data
cpi <- vroom::vroom(here::here("data", "consumerpriceindex.csv"))

cpi <- cpi %>% 
        rename(cpi = CPALTT01USQ657N)
#glimpse
glimpse(cpi)

#join the data using a left join
lc_with_bonds_cpis <- lc_with_bonds %>%
  left_join(cpi, by = c("issue_d" = "DATE")) %>%
  arrange(issue_d) %>%
  filter(!is.na(cpi))

#add cpi as explanatory variable to our final model
finalmodel_cpi<-train(
    int_rate ~ loan_amnt + term + dti + grade + Price + Price*grade + cpi, #final model with consumer price index
    lc_with_bonds_cpis,
   method = "lm",
    trControl = control
   )

summary(finalmodel_cpi)

```


>Adding consumer price index data to our final mode, we find another model with high explanatory power with an adj R-squared of 0.938 and an residual standard error of 0.009045 - this is only marginally better than our final model and for computational purposes it may still be better to assume our previous final model as best model.


```{r, final_model_plus_inflation}
#load data
inflation <- vroom::vroom(here::here("data", "inflation.csv"))

inflation <- inflation %>% 
        rename(inflation = FPCPITOTLZGUSA) %>% 
  # create year column as inflation rates are per year
        mutate(year = format(as.Date(DATE, format="%Y-%m-%d"),"%Y")) 

# create year column as inflation rates are per yea
lc_with_bonds_cpis <- lc_with_bonds_cpis %>% 
  mutate(year = format(as.Date(issue_d, format="%Y-%m-%d"),"%Y")) 
  

#glimpse
glimpse(inflation)

#join the data using a left join
lc_with_bonds_cpis_inflation <- lc_with_bonds_cpis %>%
  left_join(inflation, by = c("year" = "year")) %>%
  arrange(issue_d) %>%
  filter(!is.na(cpi)) 

#add inflation as explanatory variable to our final model
finalmodel_inflation <- train(
    int_rate ~ loan_amnt + term + dti + grade + Price + Price*grade + inflation, #final model with inflation data
    lc_with_bonds_cpis_inflation,
   method = "lm",
    trControl = control
   )

summary(finalmodel_inflation)


```

>We couldn't significantly improve our final model by adding inflation. Adding inflation does yield a slightly lower RMSE; however, it has also made a number of other factors insignificant (factors that previously were significant). This may be due to multicolinearity or heteroskedasticity issues. The reduced RMSE does improve the predictive interval for the model however this is only marginally and for simplicity and computational sake (Occam's Razor) We decide to not include inflation in our model.

**Moving forward, we could look for further data in the economics and business environment that influences interest rates significantly to help us to further improve our model and its predictive accuracy. We could also consider a series of different transformations on the existing data we have.**



